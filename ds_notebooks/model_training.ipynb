{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed8c2c7-b83b-483a-9bc7-c5c1f035e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting optuna\n",
      "  Downloading optuna-3.6.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Using cached alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from optuna) (24.1)\n",
      "Collecting sqlalchemy>=1.3.0 (from optuna)\n",
      "  Downloading SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML (from optuna)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting typing-extensions>=4 (from alembic>=1.5.0->optuna)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting MarkupSafe>=0.9.2 (from Mako->alembic>=1.5.0->optuna)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Downloading optuna-3.6.1-py3-none-any.whl (380 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m380.1/380.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached alembic-1.13.1-py3-none-any.whl (233 kB)\n",
      "Downloading SQLAlchemy-2.0.30-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Installing collected packages: typing-extensions, tqdm, PyYAML, MarkupSafe, colorlog, sqlalchemy, Mako, alembic, optuna\n",
      "Successfully installed Mako-1.3.5 MarkupSafe-2.1.5 PyYAML-6.0.1 alembic-1.13.1 colorlog-6.8.2 optuna-3.6.1 sqlalchemy-2.0.30 tqdm-4.66.4 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7945d4c0-0ae8-4522-9b09-b14ee4284147",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555069e3-53f7-4f4e-882d-bd7a41d2c5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = \"team1-index-predictor-bucket\"\n",
    "\n",
    "root_folder = \"data/processed\"\n",
    "\n",
    "train_raw_data_filename = f\"{root_folder}/train.csv\"\n",
    "validation_raw_data_filename = f\"{root_folder}/validation.csv\"\n",
    "test_raw_data_filename = f\"{root_folder}/test.csv\"\n",
    "inference_raw_data_filename = f\"{root_folder}/inference.csv\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ab267f0-433a-4dfd-b7b9-a517568edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_s3_object = s3.get_object(Bucket=BUCKET_NAME, Key=train_raw_data_filename)\n",
    "validation_s3_object = s3.get_object(\n",
    "    Bucket=BUCKET_NAME, Key=validation_raw_data_filename\n",
    ")\n",
    "test_s3_object = s3.get_object(Bucket=BUCKET_NAME, Key=test_raw_data_filename)\n",
    "\n",
    "train_data = train_s3_object[\"Body\"].read().decode(\"utf-8\")\n",
    "train_df = pd.read_csv(StringIO(train_data))\n",
    "\n",
    "validation_data = validation_s3_object[\"Body\"].read().decode(\"utf-8\")\n",
    "validation_df = pd.read_csv(StringIO(validation_data))\n",
    "\n",
    "test_data = test_s3_object[\"Body\"].read().decode(\"utf-8\")\n",
    "test_df = pd.read_csv(StringIO(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "5066e5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/processed/train.csv\")\n",
    "validation_df = pd.read_csv(\"../data/processed/validation.csv\")\n",
    "test_df = pd.read_csv(\"../data/processed/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "53105287-17e3-4b98-a0f6-a2206439b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lag_features(df, lag=1):\n",
    "    for i in range(1, lag + 1):\n",
    "        df[f\"lag_{i}\"] = df[\"Close\"].shift(i)\n",
    "    df.dropna(inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f10e9392",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(1, 64, batch_first=True)\n",
    "        self.linear = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Get the output of the last timestep\n",
    "        out = self.linear(lstm_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b4f02904",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag = 30\n",
    "\n",
    "train_df = create_lag_features(train_df, lag)\n",
    "validation_df = create_lag_features(validation_df, lag)\n",
    "test_df = create_lag_features(test_df, lag)\n",
    "\n",
    "model = LSTMModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4f31c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(model, train_df, validation_df, lag, n_epochs=10, lr=0.001):\n",
    "    X_train = train_df.drop(columns=[\"Close\"])\n",
    "    y_train = train_df[\"Close\"]\n",
    "\n",
    "    X_validation = validation_df.drop(columns=[\"Close\"])\n",
    "    y_validation = validation_df[\"Close\"]\n",
    "\n",
    "    X_train = X_train.values.reshape(-1, lag + 7, 1)\n",
    "    X_validation = X_validation.values.reshape(-1, lag + 7, 1)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(torch.Tensor(X_train))\n",
    "        loss = criterion(outputs, torch.Tensor(y_train.values).view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = model(torch.Tensor(X_validation))\n",
    "                val_loss = criterion(\n",
    "                    outputs, torch.Tensor(y_validation.values).view(-1, 1)\n",
    "                )\n",
    "                print(\n",
    "                    f\"Epoch {epoch} - Loss: {loss.item()} - Val Loss: {val_loss.item()}\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bd897b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Loss: 0.19946429133415222 - Val Loss: 0.6568797826766968\n",
      "Epoch 10 - Loss: 0.08037499338388443 - Val Loss: 0.35540950298309326\n",
      "Epoch 20 - Loss: 0.04074123129248619 - Val Loss: 0.05274505540728569\n",
      "Epoch 30 - Loss: 0.027115914970636368 - Val Loss: 0.16485995054244995\n",
      "Epoch 40 - Loss: 0.02264607883989811 - Val Loss: 0.10754813253879547\n",
      "Epoch 50 - Loss: 0.01985975354909897 - Val Loss: 0.07121189683675766\n",
      "Epoch 60 - Loss: 0.015204137191176414 - Val Loss: 0.06065426021814346\n",
      "Epoch 70 - Loss: 0.007279721554368734 - Val Loss: 0.0023650529328733683\n",
      "Epoch 80 - Loss: 0.004458197392523289 - Val Loss: 0.018891694024205208\n",
      "Epoch 90 - Loss: 0.002273330232128501 - Val Loss: 0.00039925932651385665\n",
      "Epoch 100 - Loss: 0.0015602742787450552 - Val Loss: 0.0004249253252055496\n",
      "Epoch 110 - Loss: 0.0013858929742127657 - Val Loss: 0.0003990131081081927\n",
      "Epoch 120 - Loss: 0.0012682595988735557 - Val Loss: 0.0010976779740303755\n",
      "Epoch 130 - Loss: 0.0011993483640253544 - Val Loss: 0.0007781537133269012\n",
      "Epoch 140 - Loss: 0.0011398065835237503 - Val Loss: 0.0005942031275480986\n",
      "Epoch 150 - Loss: 0.001084587536752224 - Val Loss: 0.0005785697721876204\n",
      "Epoch 160 - Loss: 0.0010373450350016356 - Val Loss: 0.0004339617444202304\n",
      "Epoch 170 - Loss: 0.0009976242436096072 - Val Loss: 0.00041562211117707193\n",
      "Epoch 180 - Loss: 0.0009646397083997726 - Val Loss: 0.0003872076340485364\n",
      "Epoch 190 - Loss: 0.000938155222684145 - Val Loss: 0.00037803727900609374\n",
      "Epoch 200 - Loss: 0.0009177961037494242 - Val Loss: 0.00037412228994071484\n",
      "Epoch 210 - Loss: 0.0009027788182720542 - Val Loss: 0.0003745351277757436\n",
      "Epoch 220 - Loss: 0.0008919649990275502 - Val Loss: 0.0003774802607949823\n",
      "Epoch 230 - Loss: 0.0008841452072374523 - Val Loss: 0.000381214456865564\n",
      "Epoch 240 - Loss: 0.0008782593649812043 - Val Loss: 0.000385216495487839\n",
      "Epoch 250 - Loss: 0.0008735272567719221 - Val Loss: 0.0003882670134771615\n",
      "Epoch 260 - Loss: 0.0008694572024978697 - Val Loss: 0.00039094313979148865\n",
      "Epoch 270 - Loss: 0.0008657821454107761 - Val Loss: 0.0003926213539671153\n",
      "Epoch 280 - Loss: 0.0008623766480013728 - Val Loss: 0.00039379121153615415\n",
      "Epoch 290 - Loss: 0.0008591888472437859 - Val Loss: 0.0003944768395740539\n",
      "Epoch 300 - Loss: 0.0008561966824345291 - Val Loss: 0.00039484765147790313\n",
      "Epoch 310 - Loss: 0.0008533862419426441 - Val Loss: 0.00039511831710115075\n",
      "Epoch 320 - Loss: 0.0008507446036674082 - Val Loss: 0.0003953251289203763\n",
      "Epoch 330 - Loss: 0.0008482562261633575 - Val Loss: 0.0003955400316044688\n",
      "Epoch 340 - Loss: 0.0008459053351543844 - Val Loss: 0.0003957993467338383\n",
      "Epoch 350 - Loss: 0.0008436751668341458 - Val Loss: 0.00039610679959878325\n",
      "Epoch 360 - Loss: 0.0008415495976805687 - Val Loss: 0.00039646224468015134\n",
      "Epoch 370 - Loss: 0.0008395133190788329 - Val Loss: 0.0003968623641412705\n",
      "Epoch 380 - Loss: 0.0008375514298677444 - Val Loss: 0.0003972889971919358\n",
      "Epoch 390 - Loss: 0.0008356508915312588 - Val Loss: 0.00039773245225660503\n"
     ]
    }
   ],
   "source": [
    "train_lstm(model, train_df, validation_df, lag, n_epochs=400, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5c3f754c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0002928580797743052\n"
     ]
    }
   ],
   "source": [
    "X_test = test_df.drop(columns=[\"Close\"])\n",
    "y_test = test_df[\"Close\"]\n",
    "\n",
    "X_test = X_test.values.reshape(-1, lag + 7, 1)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(torch.Tensor(X_test))\n",
    "    test_loss = criterion(outputs, torch.Tensor(y_test.values).view(-1, 1))\n",
    "    print(f\"Test Loss: {test_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9f5cbaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler params should be loaded from feature store\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "\n",
    "with open(\"../data/processed/scaler_params.json\", \"r\") as f:\n",
    "    scaler_params = json.load(f)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.min_, scaler.scale_ = scaler_params[\"min_\"], scaler_params[\"scale_\"]\n",
    "scaler.data_min_, scaler.data_max_ = (\n",
    "    scaler_params[\"data_min\"],\n",
    "    scaler_params[\"data_max\"],\n",
    ")\n",
    "scaler.data_range_ = scaler_params[\"data_range\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "413d1ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 18.81880458328459\n",
      "MAPE: 0.0005960154743389435\n"
     ]
    }
   ],
   "source": [
    "y_test_inv = scaler.inverse_transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "outputs_inv = scaler.inverse_transform(outputs.numpy().reshape(-1, 1)).flatten()\n",
    "\n",
    "mse = mean_squared_error(y_test_inv, outputs_inv)\n",
    "mape = mean_absolute_percentage_error(y_test_inv, outputs_inv)\n",
    "\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"MAPE: {mape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "f6bea10e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5414.91992188, 5414.66015625, 5413.37988281, 5410.81982422,\n",
       "        5411.45019531]),\n",
       " array([5412.3457, 5413.9897, 5415.487 , 5416.999 , 5418.139 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_inv[:5], outputs_inv[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d3f7b4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"lstm_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe36e9-4f3a-47be-b9ae-0b358bd32d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After model is trained, should be put to model registry"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
