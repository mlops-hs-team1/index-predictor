{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd37d6e0",
   "metadata": {},
   "source": [
    "#### Model training and evaluation notebook, tests XGBoost, Random Forest, and compares with Moving Averages baseline approach includes mlflow experiment tracking.\n",
    "\n",
    "Note that in pipeline training and evaluation is performed as pipeline steps in sagemaker jobs, this one is for experimentation and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394b8241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q optuna mlflow==2.13.2 sagemaker-mlflow==0.1.0\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from optuna import create_study\n",
    "import mlflow\n",
    "import logging\n",
    "import sagemaker\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e78b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = \"data/processed\"\n",
    "\n",
    "data_filename = \"data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446e67aa",
   "metadata": {},
   "source": [
    "#### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0e86cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"../{root_folder}/{data_filename}\")\n",
    "\n",
    "train_df = df[df[\"type\"] == \"train\"]\n",
    "validation_df = df[df[\"type\"] == \"validation\"]\n",
    "test_df = df[df[\"type\"] == \"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d2e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df[\"close_target\"]\n",
    "X_train = train_df.drop(columns=[\"close_target\", \"datetime\", \"type\", \"version\"])\n",
    "\n",
    "y_validation = validation_df[\"close_target\"]\n",
    "X_validation = validation_df.drop(\n",
    "    columns=[\"close_target\", \"datetime\", \"type\", \"version\"]\n",
    ")\n",
    "\n",
    "y_test = test_df[\"close_target\"]\n",
    "X_test = test_df.drop(columns=[\"close_target\", \"datetime\", \"type\", \"version\"])\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dvalidation = xgb.DMatrix(X_validation, label=y_validation)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40efb7a3",
   "metadata": {},
   "source": [
    "#### Constants setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb3fc7b7-bf1e-44ee-b634-f0a1c7d8a7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_server_arn = (\n",
    "    \"arn:aws:sagemaker:eu-central-1:567821811420:mlflow-tracking-server/wildfire-mj\"\n",
    ")\n",
    "experiment_name = \"index-predictor-model-training-notebook\"\n",
    "\n",
    "model_filename = \"model_xgb_v0_0_1.xgb\"\n",
    "model_folder = \"models\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ce6469",
   "metadata": {},
   "source": [
    "#### Custom metrics computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c76cd62-f948-4657-aa63-351d1536472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_reward(y_pred, close_prices):\n",
    "    \"\"\"Get the cumulative, reward, since the model predicts if at time t+3 the price will be higher\n",
    "    or lower than at time t, if model predicts correctly, we get the difference between the price at\n",
    "    time t+3 and t, if the model predicts incorrectly, we get the negative difference between the\n",
    "    price at time t+3 and t\"\"\"\n",
    "    rewards = []\n",
    "    for i in range(0, len(close_prices) - 3):\n",
    "        if y_pred[i] == 1:\n",
    "            rewards.append(close_prices[i + 3] - close_prices[i])\n",
    "        else:\n",
    "            rewards.append(close_prices[i] - close_prices[i + 3])\n",
    "    return np.sum(rewards), rewards\n",
    "\n",
    "\n",
    "def compute_cumulative_return(y_pred, close_prices):\n",
    "    \"\"\"Similar to the compute_cumulative_reward function, but in percentage terms\"\"\"\n",
    "    rewards = []\n",
    "    for i in range(0, len(close_prices) - 3):\n",
    "        if y_pred[i] == 1:\n",
    "            rewards.append((close_prices[i + 3] - close_prices[i]) / close_prices[i])\n",
    "        else:\n",
    "            rewards.append((close_prices[i] - close_prices[i + 3]) / close_prices[i])\n",
    "    return np.sum(rewards), rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "539dc2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%mkdir -p ../models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6e1340",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42d7d5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'learning_rate': 0.00730110583727178, 'num_boost_round': 55}\n",
      "Validation Accuracy: 56.11%\n",
      "Test Accuracy: 55.46%\n",
      "Cumulative return: 0.02824741766309955\n",
      "Cumulative reward: 154.76953125\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "optuna_logger.setLevel(logging.WARNING)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=sagemaker.utils.name_from_base(\"train-xgboost-optuna\")\n",
    ") as run:\n",
    "\n",
    "    def objective(trial):\n",
    "\n",
    "        params = {\n",
    "            \"objective\": \"binary:logistic\",\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 7),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 1e-1, log=True),\n",
    "            \"eval_metric\": \"logloss\",\n",
    "        }\n",
    "        num_boost_round = trial.suggest_int(\"num_boost_round\", 50, 100)\n",
    "\n",
    "        evals = [(dtrain, \"train\"), (dvalidation, \"eval\")]\n",
    "\n",
    "        bst = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=num_boost_round,\n",
    "            evals=evals,\n",
    "            early_stopping_rounds=20,\n",
    "            verbose_eval=False,\n",
    "        )\n",
    "\n",
    "        y_pred_validation = bst.predict(dvalidation)\n",
    "        y_pred_validation_binary = (y_pred_validation > 0.5).astype(int)\n",
    "\n",
    "        validation_accuracy = accuracy_score(y_validation, y_pred_validation_binary)\n",
    "\n",
    "        return validation_accuracy\n",
    "\n",
    "    study = create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    params = {\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"max_depth\": best_params[\"max_depth\"],\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"eval_metric\": \"logloss\",\n",
    "    }\n",
    "\n",
    "    evals = [(dtrain, \"train\"), (dvalidation, \"eval\")]\n",
    "\n",
    "    bst = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=best_params[\"num_boost_round\"],\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False,\n",
    "    )\n",
    "\n",
    "    y_pred_validation = bst.predict(dvalidation)\n",
    "    y_pred_test = bst.predict(dtest)\n",
    "\n",
    "    y_pred_validation_binary = (y_pred_validation > 0.5).astype(int)\n",
    "    y_pred_test_binary = (y_pred_test > 0.5).astype(int)\n",
    "\n",
    "    validation_accuracy = accuracy_score(y_validation, y_pred_validation_binary)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test_binary)\n",
    "\n",
    "    cumulative_return, return_ = compute_cumulative_return(\n",
    "        y_pred_test_binary, test_df[\"close\"].values\n",
    "    )\n",
    "    cumulative_reward, reward_ = compute_cumulative_reward(\n",
    "        y_pred_test_binary, test_df[\"close\"].values\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"train_dataset_size\": len(y_train),\n",
    "            \"validation_dataset_size\": len(y_validation),\n",
    "            \"test_dataset_size\": len(y_test),\n",
    "            \"number_of_optuna_trials\": 10,\n",
    "            \"final_model_hyperparams\": best_params,\n",
    "        },\n",
    "        run_id=run.info.run_id,\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"validation_accuracy\": validation_accuracy * 100,\n",
    "            \"test_accuracy\": test_accuracy * 100,\n",
    "            \"cumulative_return\": cumulative_return,\n",
    "            \"cumulative_reward\": cumulative_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    bst.save_model(f\"../{model_folder}/{model_filename}\")\n",
    "    mlflow.log_artifact(\n",
    "        f\"{os.path.pardir}/{model_folder}\", model_filename, run_id=run.info.run_id\n",
    "    )\n",
    "\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(\n",
    "        \"Cumulative return:\",\n",
    "        cumulative_return,\n",
    "    )\n",
    "    print(\n",
    "        \"Cumulative reward:\",\n",
    "        cumulative_reward,\n",
    "    )\n",
    "\n",
    "    mlflow.end_run(status=\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b69f12",
   "metadata": {},
   "source": [
    "#### Plot cumulative return of the model during one day of trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf95fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns = pd.Series(return_).cumsum()\n",
    "\n",
    "plot_df = pd.DataFrame(\n",
    "    {\"Index\": range(len(cumulative_returns)), \"Cumulative Return\": cumulative_returns}\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(6, 3))\n",
    "plt.plot(plot_df[\"Index\"], plot_df[\"Cumulative Return\"], label=\"Cumulative Return\")\n",
    "plt.xlabel(\"Number of minutes since start of trading day\")\n",
    "plt.ylabel(\"Cumulative return\")\n",
    "plt.title(\"Cumulative return of the model during one day of trading\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62169cee",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd78ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 113, 'max_depth': 7, 'min_samples_split': 4, 'min_samples_leaf': 4, 'max_features': 'sqrt'}\n",
      "Validation Accuracy: 55.56%\n",
      "Test Accuracy: 54.34%\n",
      "Cumulative return: 0.017248126928588882\n",
      "Cumulative reward: 94.5908203125\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import optuna\n",
    "from optuna import create_study\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "import sagemaker\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "optuna_logger = logging.getLogger(\"optuna\")\n",
    "optuna_logger.setLevel(logging.WARNING)\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=sagemaker.utils.name_from_base(\"train-rf-optuna\")\n",
    ") as run:\n",
    "\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 4),\n",
    "            \"max_features\": trial.suggest_categorical(\"max_features\", [\"sqrt\", \"log2\"]),\n",
    "        }\n",
    "\n",
    "        rf = RandomForestClassifier(**params)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        y_pred_validation = rf.predict(X_validation)\n",
    "        validation_accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "\n",
    "        return validation_accuracy\n",
    "\n",
    "    study = create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    print(study.best_params)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    rf = RandomForestClassifier(**best_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_validation = rf.predict(X_validation)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "\n",
    "    validation_accuracy = accuracy_score(y_validation, y_pred_validation)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "    cumulative_return, _ = compute_cumulative_return(\n",
    "        y_pred_test, test_df[\"close\"].values\n",
    "    )\n",
    "    cumulative_reward, _ = compute_cumulative_reward(\n",
    "        y_pred_test, test_df[\"close\"].values\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"train_dataset_size\": len(y_train),\n",
    "            \"validation_dataset_size\": len(y_validation),\n",
    "            \"test_dataset_size\": len(y_test),\n",
    "            \"number_of_optuna_trials\": 10,\n",
    "            \"final_model_hyperparams\": best_params,\n",
    "        },\n",
    "        run_id=run.info.run_id,\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"validation_accuracy\": validation_accuracy * 100,\n",
    "            \"test_accuracy\": test_accuracy * 100,\n",
    "            \"cumulative_return\": cumulative_return,\n",
    "            \"cumulative_reward\": cumulative_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_path = f\"../{model_folder}/{model_filename}\"\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(rf, f)\n",
    "    mlflow.log_artifact(model_path, run_id=run.info.run_id)\n",
    "\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(\"Cumulative return:\", cumulative_return)\n",
    "    print(\"Cumulative reward:\", cumulative_reward)\n",
    "\n",
    "    mlflow.end_run(status=\"FINISHED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80f6127",
   "metadata": {},
   "source": [
    "### Moving averages (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98a46619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size: 8\n",
      "Validation Accuracy: 51.67%\n",
      "Test Accuracy: 50.98%\n",
      "Test Cumulative return: 0.012116591910712194\n",
      "Test Cumulative reward: 66.328125\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import logging\n",
    "import sagemaker\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "mlflow.set_tracking_uri(tracking_server_arn)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "\n",
    "def moving_average_signal(close_prices, window):\n",
    "    ma = close_prices.rolling(window=window).mean()\n",
    "    signals = np.where(close_prices > ma, 1, 0)\n",
    "    return signals\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, actuals):\n",
    "    accuracy = accuracy_score(actuals, predictions)\n",
    "    cumulative_return, _ = compute_cumulative_return(\n",
    "        predictions, test_df[\"close\"].values\n",
    "    )\n",
    "    cumulative_reward, _ = compute_cumulative_reward(\n",
    "        predictions, test_df[\"close\"].values\n",
    "    )\n",
    "    return accuracy, cumulative_return, cumulative_reward\n",
    "\n",
    "\n",
    "fixed_window_size = 8\n",
    "\n",
    "with mlflow.start_run(\n",
    "    run_name=sagemaker.utils.name_from_base(\"moving-averages\")\n",
    ") as run:\n",
    "\n",
    "    y_pred_validation = moving_average_signal(X_validation[\"close\"], fixed_window_size)\n",
    "    y_pred_test = moving_average_signal(X_test[\"close\"], fixed_window_size)\n",
    "\n",
    "    validation_accuracy, validation_cumulative_return, validation_cumulative_reward = (\n",
    "        compute_metrics(y_pred_validation, y_validation)\n",
    "    )\n",
    "    test_accuracy, test_cumulative_return, test_cumulative_reward = compute_metrics(\n",
    "        y_pred_test, y_test\n",
    "    )\n",
    "\n",
    "    mlflow.log_params(\n",
    "        {\n",
    "            \"train_dataset_size\": len(y_train),\n",
    "            \"validation_dataset_size\": len(y_validation),\n",
    "            \"test_dataset_size\": len(y_test),\n",
    "            \"fixed_window_size\": fixed_window_size,\n",
    "        },\n",
    "        run_id=run.info.run_id,\n",
    "    )\n",
    "\n",
    "    mlflow.log_metrics(\n",
    "        {\n",
    "            \"validation_accuracy\": validation_accuracy * 100,\n",
    "            \"test_accuracy\": test_accuracy * 100,\n",
    "            \"test_cumulative_return\": test_cumulative_return,\n",
    "            \"test_cumulative_reward\": test_cumulative_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Window size:\", fixed_window_size)\n",
    "    print(f\"Validation Accuracy: {validation_accuracy*100:.2f}%\")\n",
    "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(\"Test Cumulative return:\", test_cumulative_return)\n",
    "    print(\"Test Cumulative reward:\", test_cumulative_reward)\n",
    "\n",
    "    mlflow.end_run(status=\"FINISHED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e294cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
